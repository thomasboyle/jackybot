import discord
import asyncio
import os
import requests
import random
import io
import tempfile
import torch
import youtube_dl
import aiohttp
import time
import gc
from PIL import Image
from discord.ext import commands
from gtts import gTTS
from bs4 import BeautifulSoup
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
from discord.voice_client import VoiceClient
from PIL import Image, ImageDraw, ImageFont
from concurrent.futures import ThreadPoolExecutor
import json
from PIL import Image, ImageDraw, ImageFont
import threading


# Initialize global variables
pipe = None
last_call_time = None
unload_model_lock = threading.Lock()
load_model_lock = threading.Lock()


class ImageGenerationTask:
    def __init__(self, prompt, ctx):
        self.prompt = prompt
        self.ctx = ctx

    # Make the ImageGenerationTask serializable
    def to_dict(self):
        return {
            'prompt': self.prompt,
            'channel_id': self.ctx.channel.id,  # Save the channel_id from the context
            'author_id': self.ctx.author.id,  # Save the author_id from the context
            'message_id': self.ctx.message.id,  # Save the message_id from the context
        }

    @staticmethod
    async def from_dict(source, bot):
        # Recreate the context object
        channel = bot.get_channel(source['channel_id'])
        author = await bot.fetch_user(source['author_id'])
        message = await channel.fetch_message(source['message_id'])
        ctx = type('Context', (), {'channel': channel, 'author': author, 'message': message})()
        return ImageGenerationTask(source['prompt'], ctx)
    
async def load_queue_from_file(bot):
    try:
        with open('queue.json', 'r') as f:
            content = f.read().strip()
            if not content:
                return []  # Return an empty queue if the file is empty
            tasks = json.loads(content)
            # Use asyncio.gather to asynchronously load all tasks
            return await asyncio.gather(*[ImageGenerationTask.from_dict(task, bot) for task in tasks])
    except Exception as e:  # Catch any exception, not just FileNotFoundError
        print(f"Failed to load queue from file: {e}")
        return []  # Return an empty queue

# When adding an item to the queue
async def generate(ctx, *, prompt):
    task = ImageGenerationTask(prompt, ctx)
    bot.image_generation_queue.append(task)
    with open('queue.json', 'w') as f:
        json.dump([task.to_dict() for task in bot.image_generation_queue], f)
    await ctx.send(f"Added {prompt} to generation queue...")

# When removing an item from the queue
async def process_image_generation_queue(bot):
    while True:
        if bot.image_generation_queue:
            task = bot.image_generation_queue.pop(0)
            with open('queue.json', 'w') as f:
                json.dump([task.to_dict() for task in bot.image_generation_queue], f)
            await generate_and_send_image(task.prompt, task.ctx, bot)
        else:
            await asyncio.sleep(1)  # sleep if the queue is empty

async def generate_and_send_image(prompt, ctx, bot):

  # Generate image
  with ThreadPoolExecutor() as pool:
    image_path = await bot.loop.run_in_executor(pool, generate_image, prompt)

  # Check if original message is available
  if ctx.message and ctx.message.channel:

    try:

      # Send as spoiler
      spoiler = discord.File(image_path, spoiler=True)
      await ctx.message.reply("Here's the image you requested:", file=spoiler)

    except discord.errors.HTTPException as e:

      if e.status == 400 and e.code == 50035:
        print("Invalid Form Body - Image too large or other issue.")
      else:  
        raise e

    except Exception as e:
    
      print(f"Error sending message: {e}")

  else:

    print("Original message not available.")
    
  # Send the image to the other channel.
  other_channel = bot.get_channel(1133393726060888135)
  if other_channel is not None:
      title = f"Prompt: {prompt}"[:256]  # Truncate the title if it's too long
      server_name = ctx.guild.name if ctx.guild else "Unknown Server"  # Get server name or use "Unknown Server" if not in a guild
      embed = discord.Embed(title=title, description=f"Generated by: {ctx.author.name}\nServer: {server_name}", color=0x00ff00)
      file = discord.File(image_path, filename="image.png")
      embed.set_image(url="attachment://image.png")
      try:
          await other_channel.send(embed=embed, file=file)
      except discord.errors.HTTPException as e:
          # Handle the HTTPException as needed
          if e.status == 400 and e.code == 50035:
              print("Invalid Form Body - Image too large or other issue.")
          else:
              raise e
  else:
      print("Couldn't find the channel with ID 1133393726060888135")

def check_and_unload_model():
    global pipe
    global last_call_time
    while True:
        time.sleep(10)  # Check every 10 seconds
        with unload_model_lock:
            if pipe is not None and time.time() - last_call_time >= 60:
                # Only clear the model if it's been more than 1 minute
                del pipe
                torch.cuda.empty_cache()
                gc.collect()
                pipe = None
                print("Model unloaded due to inactivity.")

# Start the background thread
threading.Thread(target=check_and_unload_model, daemon=True).start()

def generate_image(prompt):
    global pipe
    global last_call_time

    with unload_model_lock:
        last_call_time = time.time()

    MAX_TRIES = 3  # Number of times to retry generating the image
    tries = 0
    image = None

    while tries < MAX_TRIES:
        try:
            with load_model_lock:
                if pipe is None:
                    pipe = StableDiffusionPipeline.from_single_file(
                        "D:\\C++\\PythonPrograms\\JackyBot\\Safetensors\\dreamshaper_8.safetensors"
                    )

                    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
                    pipe = pipe.to("cuda")
                    print("Model loaded.")
                    # Print the scheduler type
                    print(f"Using scheduler: {type(pipe.scheduler)}")
            image = pipe(prompt).images[0]  # Generation step

            with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as temp:
                image.save(temp.name)
                return temp.name

        except RuntimeError as e:
            if "CUDA error: an illegal memory access was encountered" in str(e):
                print(f"Encountered a CUDA error. Attempt {tries + 1} of {MAX_TRIES}. Retrying...")
                tries += 1
                continue
            else:
                raise e
        finally:
            if image is not None:
                del image
            torch.cuda.synchronize()
            torch.cuda.empty_cache()
            gc.collect()
