import asyncio
import io
from collections import deque
import discord
from discord.ext import commands
import torch
from diffusers import ShapEPipeline
from PIL import Image
import functools

class ThreeDGenerationCog(commands.Cog):
    def __init__(self, bot):
        self.bot = bot
        self.pipe = None

    async def process_request(self, ctx, prompt):
        try:
            # Configuration settings
            guidance_scale = 15.0
            num_inference_steps = 20
            target_size = (512, 512)

            # Generate images asynchronously
            images_list = await self.generate_images(prompt, guidance_scale, num_inference_steps)

            # Resize images while avoiding storing large objects unnecessarily
            pil_images = [
                image.resize(target_size, Image.Resampling.LANCZOS)
                for image in images_list[0]  # Assuming single batch output
            ]

            # Create GIF and send as a file
            gif_bytes = await asyncio.to_thread(self.create_gif, pil_images)

            file = discord.File(gif_bytes, filename="3d_output.gif")
            await ctx.reply(file=file)

            # Send to specific channel asynchronously
            channel_id = 1192465868932780092
            channel = self.bot.get_channel(channel_id)
            gif_bytes.seek(0)  # Reset GIF bytes to the beginning
            file_for_channel = discord.File(gif_bytes, filename="3d_output.gif")
            await channel.send(f"Model generated by {ctx.author.mention}:", file=file_for_channel)

        except Exception as e:
            await ctx.send(f"Error: {str(e)}")

        finally:
            # Clean up memory
            torch.cuda.empty_cache()
            await self.unload_model()

    def get_pipe(self):
        if self.pipe is None:
            ckpt_id = "openai/shap-e"
            self.pipe = ShapEPipeline.from_pretrained(ckpt_id, torch_dtype=torch.float16).to("cuda")
        return self.pipe

    async def generate_images(self, prompt, guidance_scale, num_inference_steps):
        pipe = self.get_pipe()

        # Generate images asynchronously
        images_list = await asyncio.to_thread(
            pipe,
            prompt,
            guidance_scale=guidance_scale,
            num_inference_steps=num_inference_steps,
            frame_size=128,
            generator=torch.cuda.manual_seed_all(42),
        )

        return images_list.images

    @staticmethod
    def create_gif(pil_images):
        gif_bytes = io.BytesIO()
        pil_images[0].save(
            gif_bytes,
            format='GIF',
            save_all=True,
            append_images=pil_images[1:],
            duration=200,
            loop=0,
        )
        gif_bytes.seek(0)
        return gif_bytes

    async def unload_model(self):
        # Unload model and clear memory to optimize resources
        if self.pipe is not None:
            del self.pipe
            self.pipe = None
            torch.cuda.empty_cache()

async def setup(bot):
    await bot.add_cog(ThreeDGenerationCog(bot))
