import discord
import asyncio
import tempfile
import torch
import time
import gc
from diffusers import StableDiffusionPipeline
from concurrent.futures import ThreadPoolExecutor
from diffusers import DPMSolverMultistepScheduler
import json
import threading


# Initialize global variables
pipe = None
last_call_time = None
unload_model_lock = threading.Lock()
load_model_lock = threading.Lock()

class ImageGenerationTask:
    def __init__(self, prompt, ctx):
        self.prompt = prompt
        self.ctx = ctx

    # Make the ImageGenerationTask serializable
    def to_dict(self):
        return {
            'prompt': self.prompt,
            'channel_id': self.ctx.channel.id,  # Save the channel_id from the context
            'author_id': self.ctx.author.id,  # Save the author_id from the context
            'message_id': self.ctx.message.id,  # Save the message_id from the context
        }

    @staticmethod
    async def from_dict(source, bot):
        # Recreate the context object
        channel = bot.get_channel(source['channel_id'])
        author = await bot.fetch_user(source['author_id'])
        message = await channel.fetch_message(source['message_id'])
        ctx = type('Context', (), {'channel': channel, 'author': author, 'message': message})()
        return ImageGenerationTask(source['prompt'], ctx)
    
async def load_queue_from_file(bot):
    try:
        with open('queue.json', 'r') as f:
            content = f.read().strip()
            if not content:
                return []  # Return an empty queue if the file is empty
            tasks = json.loads(content)
            # Use asyncio.gather to asynchronously load all tasks
            return await asyncio.gather(*[ImageGenerationTask.from_dict(task, bot) for task in tasks])
    except Exception as e:  # Catch any exception, not just FileNotFoundError
        print(f"Failed to load queue from file: {e}")
        return []  # Return an empty queue

# When adding an item to the queue
async def generate(ctx, *, prompt):
    task = ImageGenerationTask(prompt, ctx)
    bot.image_generation_queue.append(task)
    with open('queue.json', 'w') as f:
        json.dump([task.to_dict() for task in bot.image_generation_queue], f)
    await ctx.send(f"Added {prompt} to generation queue...")

# When removing an item from the queue
async def process_image_generation_queue(bot):
    while True:
        if bot.image_generation_queue:
            task = bot.image_generation_queue.pop(0)
            with open('queue.json', 'w') as f:
                json.dump([task.to_dict() for task in bot.image_generation_queue], f)
            await generate_and_send_image(task.prompt, task.ctx, bot)
        else:
            await asyncio.sleep(1)  # sleep if the queue is empty

async def generate_and_send_image(prompt, ctx, bot):
    # Generate image
    with ThreadPoolExecutor() as pool:
        image_path = await bot.loop.run_in_executor(pool, generate_image, prompt)

    # Check if original message is available
    if ctx.message and ctx.message.channel:
        try:
            # Send as spoiler
            spoiler = discord.File(image_path, spoiler=True)
            await ctx.message.reply("Here's the image you requested:", file=spoiler)
        except discord.errors.HTTPException as e:
            if e.status == 400 and e.code == 50035:
                print("Invalid Form Body - Image too large or other issue.")
            else:  
                raise e
        except Exception as e:
            print(f"Error sending message: {e}")
    else:
        print("Original message not available.")
        
    # Send the image to the other channel.
    other_channel = bot.get_channel(1133393726060888135)
    if other_channel is not None:
        title = f"Prompt: {prompt}"[:256]  # Truncate the title if it's too long
        server_name = ctx.guild.name if ctx.guild else "Unknown Server"  # Get server name or use "Unknown Server" if not in a guild
        embed = discord.Embed(title=title, description=f"Generated by: {ctx.author.name}\nServer: {server_name}", color=0x00ff00)
        file = discord.File(image_path, filename="image.png")
        embed.set_image(url="attachment://image.png")
        try:
            await other_channel.send(embed=embed, file=file)
        except discord.errors.HTTPException as e:
            # Handle the HTTPException as needed
            if e.status == 400 and e.code == 50035:
                print("Invalid Form Body - Image too large or other issue.")
            else:
                raise e
    else:
        print("Couldn't find the channel with ID 1133393726060888135")

def check_and_unload_model():
    global pipe
    global last_call_time
    while True:
        time.sleep(10)  # Check every 10 seconds
        with unload_model_lock:
            if pipe is not None and time.time() - last_call_time >= 60:
                # Only clear the model if it's been more than 1 minute
                del pipe
                torch.cuda.empty_cache()
                gc.collect()
                pipe = None
                print("Model unloaded due to inactivity.")

# Start the background thread
threading.Thread(target=check_and_unload_model, daemon=True).start()

def generate_image(prompt, num_inference_steps=30):
    global pipe, last_call_time
    with unload_model_lock:
        last_call_time = time.time()
        MAX_TRIES = 3  # Number of times to retry generating the image
        tries = 0
        image = None
        while tries < MAX_TRIES:
            try:
                with load_model_lock:
                    if pipe is None:
                        pipe = StableDiffusionPipeline.from_single_file(
                            "D:\\C++\\PythonPrograms\\JackyBot\\Safetensors\\dreamshaper_8.safetensors"
                        )
                        pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config, use_karras_sigmas=True, algorithm_type="sde-dpmsolver++")
                        pipe = pipe.to("cuda")
                        pipe.enable_xformers_memory_efficient_attention()
                        print("Model loaded.")
                    # Print the scheduler type
                    print(f"Using scheduler: {type(pipe.scheduler)}")
                    image = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=7.5, height=512, width=512).images[0]
                with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as temp:
                    image.save(temp.name)
                    return temp.name
            except RuntimeError as e:
                if "CUDA error: an illegal memory access was encountered" in str(e):
                    print(f"Encountered a CUDA error. Attempt {tries + 1} of {MAX_TRIES}. Retrying...")
                    tries += 1
                    continue
                else:
                    raise e
            finally:
                if image is not None:
                    del image
                torch.cuda.synchronize()
                torch.cuda.empty_cache()
                gc.collect()